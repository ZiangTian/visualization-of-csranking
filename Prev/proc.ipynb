{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scholarly\n",
      "  Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from scholarly) (2.28.1)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "     ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/10.0 MB 409.6 kB/s eta 0:00:25\n",
      "     --------------------------------------- 0.1/10.0 MB 654.9 kB/s eta 0:00:16\n",
      "      -------------------------------------- 0.2/10.0 MB 841.6 kB/s eta 0:00:12\n",
      "     - -------------------------------------- 0.3/10.0 MB 1.0 MB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.4/10.0 MB 1.4 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.6/10.0 MB 1.7 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.6/10.0 MB 1.7 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 0.8/10.0 MB 1.8 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.2/10.0 MB 2.3 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 1.6/10.0 MB 3.0 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 1.7/10.0 MB 3.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 1.8/10.0 MB 2.8 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 1.9/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 1.9/10.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.0/10.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.1/10.0 MB 2.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.2/10.0 MB 2.6 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.3/10.0 MB 2.5 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 2.5/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 2.6/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 2.7/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 2.9/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.1/10.0 MB 2.7 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 3.3/10.0 MB 2.8 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 3.5/10.0 MB 2.9 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 3.7/10.0 MB 2.9 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.0/10.0 MB 3.0 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.2/10.0 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 4.5/10.0 MB 3.2 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 4.8/10.0 MB 3.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 5.0/10.0 MB 3.3 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 5.3/10.0 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 5.5/10.0 MB 3.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 5.9/10.0 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 6.3/10.0 MB 3.7 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 6.6/10.0 MB 3.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 6.9/10.0 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.2/10.0 MB 3.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.7/10.0 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.2/10.0 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.5/10.0 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 8.8/10.0 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.3/10.0 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  9.8/10.0 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.0/10.0 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from scholarly) (4.4.0)\n",
      "Collecting free-proxy\n",
      "  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sphinx-rtd-theme\n",
      "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "     ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/2.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.3/2.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.5/2.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.8/2.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 1.0/2.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.4/2.8 MB 5.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.9/2.8 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.5/2.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.8/2.8 MB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from scholarly) (4.12.0)\n",
      "Collecting bibtexparser\n",
      "  Downloading bibtexparser-1.4.1.tar.gz (55 kB)\n",
      "     ---------------------------------------- 0.0/55.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.1/55.1 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.4.0-py3-none-any.whl (15 kB)\n",
      "Collecting arrow\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 0.0/66.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 66.4/66.4 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "     ---------------------------------------- 0.0/75.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 75.0/75.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from arrow->scholarly) (2.8.2)\n",
      "Collecting types-python-dateutil>=2.8.10\n",
      "  Downloading types_python_dateutil-2.8.19.14-py3-none-any.whl (9.4 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from beautifulsoup4->scholarly) (2.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from bibtexparser->scholarly) (3.0.9)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from free-proxy->scholarly) (4.9.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from httpx->scholarly) (2023.7.22)\n",
      "Collecting anyio\n",
      "  Downloading anyio-4.1.0-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 0.0/83.9 kB ? eta -:--:--\n",
      "     ------------------- -------------------- 41.0/83.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.9/83.9 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from httpx->scholarly) (3.4)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 76.9/76.9 kB ? eta 0:00:00\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from requests[socks]->scholarly) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from requests[socks]->scholarly) (1.26.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.23.1-py3-none-any.whl (448 kB)\n",
      "     ---------------------------------------- 0.0/448.3 kB ? eta -:--:--\n",
      "     ------------ ------------------------- 143.4/448.3 kB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 448.3/448.3 kB 4.7 MB/s eta 0:00:00\n",
      "Collecting sphinx<8,>=5\n",
      "  Downloading sphinx-7.2.6-py3-none-any.whl (3.2 MB)\n",
      "     ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.4/3.2 MB 7.6 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.6/3.2 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.8/3.2 MB 6.5 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.0/3.2 MB 5.9 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.6/3.2 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.8/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 2.1/3.2 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 2.5/3.2 MB 6.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 2.7/3.2 MB 6.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 3.1/3.2 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.2/3.2 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting docutils<0.21\n",
      "  Downloading docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "     ---------------------------------------- 0.0/572.7 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 256.0/572.7 kB 5.2 MB/s eta 0:00:01\n",
      "     -------------------------------------  563.2/572.7 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 572.7/572.7 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-jquery<5,>=4\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "     ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "     -------------------------------------- 121.1/121.1 kB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.7-py3-none-any.whl (120 kB)\n",
      "     ---------------------------------------- 0.0/120.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 120.0/120.0 kB ? eta 0:00:00\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl (99 kB)\n",
      "     ---------------------------------------- 0.0/99.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 99.2/99.2 kB ? eta 0:00:00\n",
      "Collecting babel>=2.9\n",
      "  Downloading Babel-2.13.1-py3-none-any.whl (10.1 MB)\n",
      "     ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.3/10.1 MB 7.0 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.6/10.1 MB 6.8 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.0/10.1 MB 6.8 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.3/10.1 MB 6.9 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 1.6/10.1 MB 7.2 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.0/10.1 MB 7.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.0/10.1 MB 6.4 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.3/10.1 MB 6.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.5/10.1 MB 6.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 2.7/10.1 MB 5.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 2.9/10.1 MB 5.7 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.1/10.1 MB 5.6 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 3.3/10.1 MB 5.6 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 3.6/10.1 MB 5.5 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 3.8/10.1 MB 5.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.1/10.1 MB 5.5 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 4.3/10.1 MB 5.5 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 4.6/10.1 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 4.9/10.1 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.2/10.1 MB 5.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.5/10.1 MB 5.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.6/10.1 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.7/10.1 MB 5.4 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 5.8/10.1 MB 5.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 5.9/10.1 MB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.0/10.1 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 6.1/10.1 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 6.2/10.1 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.4/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 6.5/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 6.7/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 6.9/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.1/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.2/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 7.5/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 7.7/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 7.9/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 8.2/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 8.4/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.7/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 8.9/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.0/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.0/10.1 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.3/10.1 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 9.8/10.1 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.1/10.1 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.1/10.1 MB 4.7 MB/s eta 0:00:00\n",
      "Collecting imagesize>=1.3\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl (92 kB)\n",
      "     ---------------------------------------- 0.0/92.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.7/92.7 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.5-py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 0.0/83.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 83.5/83.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (3.1.2)\n",
      "Collecting Pygments>=2.14\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 0.3/1.2 MB 5.2 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.5/1.2 MB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 0.8/1.2 MB 5.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 1.1/1.2 MB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 5.3 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.6-py3-none-any.whl (89 kB)\n",
      "     ---------------------------------------- 0.0/89.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 89.4/89.4 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting snowballstemmer>=2.0\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 0.0/93.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 93.0/93.0 kB 5.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (23.0)\n",
      "Requirement already satisfied: colorama>=0.4.5 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (22.1.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.15.1)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benjamin\\.conda\\envs\\nlp\\lib\\site-packages (from Jinja2>=3.0->sphinx<8,>=5->sphinx-rtd-theme->scholarly) (2.1.1)\n",
      "Building wheels for collected packages: bibtexparser, free-proxy\n",
      "  Building wheel for bibtexparser (setup.py): started\n",
      "  Building wheel for bibtexparser (setup.py): finished with status 'done'\n",
      "  Created wheel for bibtexparser: filename=bibtexparser-1.4.1-py3-none-any.whl size=43274 sha256=7b4fbfaf5f2754771e2656fadd96f61dfc17b9d58cf85c7308457e93bbe35d8c\n",
      "  Stored in directory: c:\\users\\benjamin\\appdata\\local\\pip\\cache\\wheels\\08\\c6\\c3\\56e639fab68d1fdbf13ea147636d9795ccdbd3c1d3178d1332\n",
      "  Building wheel for free-proxy (setup.py): started\n",
      "  Building wheel for free-proxy (setup.py): finished with status 'done'\n",
      "  Created wheel for free-proxy: filename=free_proxy-1.1.1-py3-none-any.whl size=5663 sha256=8a164675fe5015830b2b6607c5ce7959890053aead5d62766c23e8e0a540af11\n",
      "  Stored in directory: c:\\users\\benjamin\\appdata\\local\\pip\\cache\\wheels\\5a\\96\\c7\\5a434714fff4fea9a59075428b142626e0a74f8c3bf90a50d0\n",
      "Successfully built bibtexparser free-proxy\n",
      "Installing collected packages: types-python-dateutil, sortedcontainers, snowballstemmer, fake-useragent, wrapt, sphinxcontrib-jsmath, sniffio, python-dotenv, Pygments, outcome, imagesize, h11, exceptiongroup, docutils, bibtexparser, babel, alabaster, wsproto, trio, httpcore, free-proxy, deprecated, arrow, anyio, trio-websocket, httpx, selenium, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, sphinx, sphinxcontrib-jquery, sphinx-rtd-theme, scholarly\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "Successfully installed Pygments-2.17.2 alabaster-0.7.13 anyio-4.1.0 arrow-1.3.0 babel-2.13.1 bibtexparser-1.4.1 deprecated-1.2.14 docutils-0.20.1 exceptiongroup-1.2.0 fake-useragent-1.4.0 free-proxy-1.1.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 imagesize-1.4.1 outcome-1.3.0.post0 python-dotenv-1.0.0 scholarly-1.7.11 selenium-4.16.0 sniffio-1.3.0 snowballstemmer-2.2.0 sortedcontainers-2.4.0 sphinx-7.2.6 sphinx-rtd-theme-2.0.0 sphinxcontrib-applehelp-1.0.7 sphinxcontrib-devhelp-1.0.5 sphinxcontrib-htmlhelp-2.0.4 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.6 sphinxcontrib-serializinghtml-1.1.9 trio-0.23.1 trio-websocket-0.11.1 types-python-dateutil-2.8.19.14 wrapt-1.16.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "%pip install scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('csrankings.csv')\n",
    "\n",
    "# Define keywords for each field\n",
    "system_keywords = ['system', 'architecture', 'hardware']\n",
    "ai_keywords = ['artificial intelligence', 'machine learning', 'deep learning', 'computer vision']\n",
    "theory_keywords = ['theory', 'algorithm', 'computational complexity']\n",
    "interdisciplinary_keywords = ['interdisciplinary', 'cross-disciplinary', 'multi-disciplinary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns for each field\n",
    "df['System'] = False\n",
    "df['AI'] = False\n",
    "df['Theory'] = False\n",
    "df['Interdisciplinary'] = False\n",
    "df['Publications'] = 0\n",
    "df['Citations'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check keywords in the webpage content\n",
    "def check_keywords(url, keywords):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        interests = soup.find('div', class_='interests').text  # Replace with the actual HTML structure\n",
    "        return any(keyword in interests.lower() for keyword in keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     name                      affiliation  \\\n",
      "0              A Min Tjoa                          TU Wien   \n",
      "1       A. Akbari Azirani                             IUST   \n",
      "2        A. Akbariazirani                             IUST   \n",
      "3          A. Aldo Faisal          Imperial College London   \n",
      "4      A. Antony Franklin                    IIT Hyderabad   \n",
      "...                   ...                              ...   \n",
      "28481        Özgür Erkent             Hacettepe University   \n",
      "28482        Özgür Ulusoy               Bilkent University   \n",
      "28483        Öznur Tastan               Sabancı University   \n",
      "28484       Öznur Özkasap                   Koç University   \n",
      "28485  Ümit V. Çatalyürek  Georgia Institute of Technology   \n",
      "\n",
      "                                                homepage     scholarid  \\\n",
      "0                       http://www.ifs.tuwien.ac.at/tjoa  x8qCMhcAAAAJ   \n",
      "1      http://ce.iust.ac.ir/page.php?slct_pg_id=6537&...  pCil4_cAAAAJ   \n",
      "2      http://ce.iust.ac.ir/page.php?slct_pg_id=6537&...  pCil4_cAAAAJ   \n",
      "3             https://www.imperial.ac.uk/people/a.faisal  WjHjbrwAAAAJ   \n",
      "4               http://www.iith.ac.in/~antony/index.html  LVfqLuoAAAAJ   \n",
      "...                                                  ...           ...   \n",
      "28481       https://web.cs.hacettepe.edu.tr/~ozgurerkent  5QMAbisAAAAJ   \n",
      "28482              http://www.cs.bilkent.edu.tr/~oulusoy  M_vU9pQAAAAJ   \n",
      "28483              http://people.sabanciuniv.edu/otastan  8ljjXmEAAAAJ   \n",
      "28484                    http://home.ku.edu.tr/~oozkasap  v2WiJeIAAAAJ   \n",
      "28485                    https://www.cc.gatech.edu/~umit  OLDMURQAAAAJ   \n",
      "\n",
      "       System     AI  Theory  Interdisciplinary  Publications  Citations  \n",
      "0       False  False   False              False             0          0  \n",
      "1       False  False   False              False             0          0  \n",
      "2       False  False   False              False             0          0  \n",
      "3       False  False   False              False             0          0  \n",
      "4       False  False   False              False             0          0  \n",
      "...       ...    ...     ...                ...           ...        ...  \n",
      "28481   False  False   False              False             0          0  \n",
      "28482   False  False   False              False             0          0  \n",
      "28483   False  False   False              False             0          0  \n",
      "28484   False  False   False              False             0          0  \n",
      "28485   False  False   False              False             0          0  \n",
      "\n",
      "[28486 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google_scholar(author_id, author_name):\n",
    "    try:\n",
    "        if author_id != 'NOSCHOLARPAGE':\n",
    "            author = scholarly.search_author_id(author_id)\n",
    "        else:\n",
    "            search_query = scholarly.search_author(author_name)\n",
    "            author = next(search_query)\n",
    "        return author\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for author with ID {author_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception for author with ID LVfqLuoAAAAJ: Cannot Fetch from Google Scholar.\n",
      "Exception for author with ID n25sJzkAAAAJ: Cannot Fetch from Google Scholar.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\One Drive\\OneDrive - whu.edu.cn\\Learning\\VR\\vis\\proc.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m author_id \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mscholarid\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# Replace 'GoogleScholarID' with the actual column name\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m author_name \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m author_info \u001b[39m=\u001b[39m search_google_scholar(author_id, author_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m author_info:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     df\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39mCitations\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m author_info\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcitedby\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "\u001b[1;32me:\\One Drive\\OneDrive - whu.edu.cn\\Learning\\VR\\vis\\proc.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m author_id \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNOSCHOLARPAGE\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         author \u001b[39m=\u001b[39m scholarly\u001b[39m.\u001b[39;49msearch_author_id(author_id)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/One%20Drive/OneDrive%20-%20whu.edu.cn/Learning/VR/vis/proc.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         search_query \u001b[39m=\u001b[39m scholarly\u001b[39m.\u001b[39msearch_author(author_name)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\.conda\\envs\\nlp\\lib\\site-packages\\scholarly\\_scholarly.py:349\u001b[0m, in \u001b[0;36m_Scholarly.search_author_id\u001b[1;34m(self, id, filled, sortby, publication_limit)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_author_id\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mid\u001b[39m: \u001b[39mstr\u001b[39m, filled: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, sortby: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcitedby\u001b[39m\u001b[39m\"\u001b[39m, publication_limit: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mAuthor:\n\u001b[0;32m    323\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search by author id and return a single Author object\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[39m    :param sortby: select the order of the citations in the author page. Either by 'citedby' or 'year'. Defaults to 'citedby'.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m    :type sortby: string\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39m             'url_picture': 'https://scholar.googleusercontent.com/citations?view_op=view_photo&user=EmD_lTEAAAAJ&citpid=3'}\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__nav\u001b[39m.\u001b[39;49msearch_author_id(\u001b[39mid\u001b[39;49m, filled, sortby, publication_limit)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\.conda\\envs\\nlp\\lib\\site-packages\\scholarly\\_navigator.py:316\u001b[0m, in \u001b[0;36mNavigator.search_author_id\u001b[1;34m(self, id, filled, sortby, publication_limit)\u001b[0m\n\u001b[0;32m    314\u001b[0m     res \u001b[39m=\u001b[39m author_parser\u001b[39m.\u001b[39mfill(res, sortby\u001b[39m=\u001b[39msortby, publication_limit\u001b[39m=\u001b[39mpublication_limit)\n\u001b[0;32m    315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     res \u001b[39m=\u001b[39m author_parser\u001b[39m.\u001b[39;49mfill(res, sections\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mbasics\u001b[39;49m\u001b[39m'\u001b[39;49m], sortby\u001b[39m=\u001b[39;49msortby, publication_limit\u001b[39m=\u001b[39;49mpublication_limit)\n\u001b[0;32m    317\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\.conda\\envs\\nlp\\lib\\site-packages\\scholarly\\author_parser.py:441\u001b[0m, in \u001b[0;36mAuthorParser.fill\u001b[1;34m(self, author, sections, sortby, publication_limit)\u001b[0m\n\u001b[0;32m    439\u001b[0m url_citations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sortby_str\n\u001b[0;32m    440\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m&pagesize=\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(url_citations, _PAGESIZE)\n\u001b[1;32m--> 441\u001b[0m soup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnav\u001b[39m.\u001b[39;49m_get_soup(url)\n\u001b[0;32m    443\u001b[0m \u001b[39m# Update scholar_id\u001b[39;00m\n\u001b[0;32m    444\u001b[0m scholar_id \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mfindall(_CITATIONAUTHRE, soup\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m\"\u001b[39m, rel\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcanonical\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\.conda\\envs\\nlp\\lib\\site-packages\\scholarly\\_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_soup\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BeautifulSoup:\n\u001b[0;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     html \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_page(\u001b[39m'\u001b[39;49m\u001b[39mhttps://scholar.google.com\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(url))\n\u001b[0;32m    240\u001b[0m     html \u001b[39m=\u001b[39m html\u001b[39m.\u001b[39mreplace(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\xa0\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    241\u001b[0m     res \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Benjamin\\.conda\\envs\\nlp\\lib\\site-packages\\scholarly\\_navigator.py:113\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[1;34m(self, pagerequest, premium)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     w \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39muniform(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m     time\u001b[39m.\u001b[39;49msleep(w)\n\u001b[0;32m    114\u001b[0m     resp \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(pagerequest, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m premium \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:  \u001b[39m# premium methods may contain sensitive information\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    author_id = str(row['scholarid'])  # Replace 'GoogleScholarID' with the actual column name\n",
    "    author_name = str(row['name'])\n",
    "    author_info = search_google_scholar(author_id, author_name)\n",
    "\n",
    "    if author_info:\n",
    "        df.at[index, 'Citations'] = author_info.get('citedby', 0)\n",
    "\n",
    "        interests = author_info.get('interests', [])\n",
    "        df.at[index, 'System'] = any(keyword in interests for keyword in system_keywords)\n",
    "        df.at[index, 'AI'] = any(keyword in interests for keyword in ai_keywords)\n",
    "        df.at[index, 'Theory'] = any(keyword in interests for keyword in theory_keywords)\n",
    "        df.at[index, 'Interdisciplinary'] = any(keyword in interests for keyword in interdisciplinary_keywords)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('tagged_faculty_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Author', 'filled': ['basics'], 'scholar_id': 'mF-F-zkAAAAJ', 'source': <AuthorSource.AUTHOR_PROFILE_PAGE: 'AUTHOR_PROFILE_PAGE'>, 'name': 'Anand Madhu Kumar', 'url_picture': 'https://scholar.googleusercontent.com/citations?view_op=view_photo&user=mF-F-zkAAAAJ&citpid=2', 'affiliation': 'Indie', 'interests': ['Neuroscience', 'Egyptology', 'Economics', 'Humanities'], 'email_domain': '@amonereb.com', 'homepage': 'https://www.instagram.com/drewwgie/'}\n"
     ]
    }
   ],
   "source": [
    "print(author_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basics']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_info['filled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'container_type': 'Author', 'filled': ['basics'], 'scholar_id': 'vY7MdLYAAAAJ', 'source': <AuthorSource.AUTHOR_PROFILE_PAGE: 'AUTHOR_PROFILE_PAGE'>, 'name': 'Kang G. Shin', 'url_picture': 'https://scholar.googleusercontent.com/citations?view_op=view_photo&user=vY7MdLYAAAAJ&citpid=4', 'affiliation': \"Kevin & Nancy O'Connor Professor of Computer Science, University of Michigan\", 'organization': 4770128543809686866, 'interests': ['Real-time embedded systems', 'cyber-physical systems', 'computer networks', 'security and privacy', 'mobile and wireless computing and c'], 'email_domain': '@umich.edu', 'homepage': 'http://www.eecs.umich.edu/~kgshin', 'citedby': 59065}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the author's data, fill-in, and print\n",
    "print(scholarly.search_author_id('vY7MdLYAAAAJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = scholarly.search_author(\"A. Aldo Faisal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception for author with ID NOSCHOLARPAGE: \n"
     ]
    }
   ],
   "source": [
    "author_info = search_google_scholar(\"NOSCHOLARPAGE\", 'A. T. Chamillard')\n",
    "\n",
    "if author_info:\n",
    "    df.at[index, 'Citations'] = author_info.get('citedby', 0)\n",
    "\n",
    "    interests = author_info.get('interests', [])\n",
    "    df.at[index, 'System'] = any(keyword in interests for keyword in system_keywords)\n",
    "    df.at[index, 'AI'] = any(keyword in interests for keyword in ai_keywords)\n",
    "    df.at[index, 'Theory'] = any(keyword in interests for keyword in theory_keywords)\n",
    "    df.at[index, 'Interdisciplinary'] = any(keyword in interests for keyword in interdisciplinary_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mDLQC0EAAAAJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = scholarly.search_author('A. T. Chamillard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterable is empty\n"
     ]
    }
   ],
   "source": [
    "if not any(search_query):\n",
    "    print(\"Iterable is empty\")\n",
    "else:\n",
    "    for sq in search_query:\n",
    "        print(sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterable is not empty\n"
     ]
    }
   ],
   "source": [
    "if not search_query:\n",
    "    print(\"Iterable is empty\")\n",
    "else:\n",
    "    print(\"Iterable is not empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(author)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
